{
  "cells": [
    {
      "metadata": {
        "_uuid": "6ea62539cdeb4a753fffaa8b87290150ddd29722"
      },
      "cell_type": "markdown",
      "source": "Wczytanie danych:"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom scipy.sparse import csr_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport os\nprint(os.listdir(\"../input\"))\n\nreviews = pd.read_csv('../input/train.csv', header=0, sep=',')\n\nreviews = reviews.Reviews",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2806d52491f06fac66c0c05746ed6f8502377d7b"
      },
      "cell_type": "markdown",
      "source": "Recenzje tokenizujemy oraz dodajemy do countera:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "801eb95733aef43c2aba9e099c8bc9fbad8c13c4"
      },
      "cell_type": "code",
      "source": "from nltk import word_tokenize\n\nwords = Counter()\n\nfor i in reviews.index:\n    review = reviews.iat[i]\n    if type(review) == str:\n        words.update(word_tokenize(review))\n        \nwords.most_common(15)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c875e70bd7bbf7ebb7edcce09e2c4b39d5a678ee"
      },
      "cell_type": "markdown",
      "source": "Usuwamy stop words oraz znaki interpunkcyjne:"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4e649047ee399fe187c285aae355942c9271d483"
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nimport string\n\nstopwords = set(stopwords.words('english')).union(set(string.punctuation))\n\nfor word in list(words.elements()):\n    if word.lower() in stopwords:\n        del words[word]\ndel words['']\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c29eab8b972751b40fbfa770dd900cd6a838c609"
      },
      "cell_type": "markdown",
      "source": "Przygotowujemy funkcje, która wejściowe dane zamieni na csr_matrix"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "15644d536998d8e95080338e4f010b5388b15a5a"
      },
      "cell_type": "code",
      "source": "def prepare_data(documents, features, train=False):\n    row = []\n    col = []\n    data = []\n\n    labels = []\n    \n    \n\n\n    for i in documents.index:\n        if type(documents.iloc[i, 4]) == str:\n            document_tokens = word_tokenize(documents.iloc[i, 4])\n        else:\n            document_tokens = []\n            \n        if train:\n                label = documents.iloc[i, 5]\n                labels.append(int(label) / 5)\n                \n        if len(document_tokens) == 0:\n            document_tokens = [\"\"]\n                \n        document_counter = Counter()\n        document_counter.update(document_tokens)\n            \n        for token in set(document_tokens):\n            if token not in features:\n                continue\n            row.append(i)\n            col.append(features[token])\n            data.append(document_counter[token])\n\n                \n    return csr_matrix((data, (row, col)), shape=(len(documents.index), len(features))), labels",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "19e2820a2cc5e441cac3c8521ddfb86442ab92aa"
      },
      "cell_type": "markdown",
      "source": "Wczytujemy dane treningowe i testowe oraz wybieramy tokeny na których będziemy budować model. Do tokenów dodajemy wartość pustą, która będzie odpowiadała pustej recenzji.\nZmienna local określa nam czy chcemy wygenerować rozwiązania (False) czy podzielić zbiór treningowy na treningowy oraz testowy (True)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d7175a607d237e29106ede419283c08a4e30252",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "min_word_count = 100\n\ntrain_data = pd.read_csv(\"../input/train.csv\", sep=\",\", header=0)\ntest_data = pd.read_csv(\"../input/test.csv\", sep=\",\", header=0)\n\ncommon_words = list([k for k, v in words.most_common() if v > min_word_count])\n\ncommon_words.append(\"\")\n\nprint(\"Tokens: \" + str(len(common_words)))\nfeature_dict = {}\nfor word in common_words:\n    feature_dict[word] = len(feature_dict)\n\nprint(\"Training classifier...\")\nX_train, y_train = prepare_data(train_data, feature_dict, True)\n\nlocal = False\n\nif local:\n    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\nelse:\n    X_test, empty_list = prepare_data(test_data, feature_dict)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ec0fb045ec51d2aac761fbd4f17bccf259fafc5d"
      },
      "cell_type": "markdown",
      "source": "Budujemy model, uczymy i dokonujemy predykcji. W zależności od wartości zmiennej \"local\" zapisujemy rozwiązanie do pliku lub obliczamy RMSE."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c7f2626063757b5b365685d8831edd1559f97e55"
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import LabelBinarizer\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\nmodel.add(Dense(128, input_shape=(len(common_words),), activation=\"tanh\"))\nmodel.add(Dense(64, activation=\"tanh\"))\nmodel.add(Dense(1, activation=\"tanh\"))\n\n\nmodel.compile(loss=\"mse\", optimizer='rmsprop', metrics=[\"mse\"])\n\nH = model.fit(X_train, y_train,epochs=20, batch_size=32)\n\npredictions = model.predict(X_test, batch_size=32)\n\npredicted = []\n\n\nfor i in range(len(predictions)):\n    predicted.append(predictions[i][0] * 5)\n    if predicted[i] > 5:\n        predicted[i] = 5\n    if predicted[i] < 1:\n        predicted[i] = 1\n\n        \n        \nif local:\n    for i in range(len(y_test)):\n        y_test[i] = y_test[i] * 5\n    \n    from math import sqrt\n    from sklearn.metrics import mean_squared_error\n    rmse = sqrt(mean_squared_error(y_test, predicted))\n    print(\"RMSE: \", rmse)\n\nif not local:\n    import csv\n    with open('submission.csv', 'w') as f:\n        writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n        writer.writerow([\"Id\", \"Rating\"])\n        for i in range(len(predicted)):\n            writer.writerow([test_data.iat[i,0], predicted[i]])  ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}